{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+eRnI0PJ8azDx+SKkphe2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuelhurni/ML-Cellsegmentation-HSLU-FS24/blob/feature_Sam/Sartorius_segmentation_kaggle_Sam_v001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Sartorius - Cell Instance Segmentation\n",
        "##Detect single neuronal cells in microscopy images\n",
        "\n",
        "Project HSLU Master IT Digitalization & Sustainability\n",
        "Module: Machine Learning and Data Science\n",
        "* Samuel Hurni\n",
        "* Pradanendr Sudev  \n",
        "* Chakravarti Devanandini\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l6zoX0SeIhBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 General information and references\n",
        "\n",
        "Used Third party Libraries:\n",
        "* Pytorch\n",
        "* TQDM\n",
        "* Pandas\n",
        "* Numpy\n",
        "* gdown\n",
        "* Matplotlib\n",
        "\n",
        "Used Thid party Imports:\n",
        "* Auxiliary functions metric: \"https://www.kaggle.com/code/theoviel/competition-metric-map-iou\n",
        "* Auxiliary functions for encoding and decoding the mask: \"https://www.kaggle.com/code/enzou3/sartorius-mask-r-cnn\"\n",
        "\n",
        "\n",
        "\n",
        "References to Turtorials / Code documantation:\n",
        "* Pytorch documentation: https://pytorch.org/docs/stable/index.html\n",
        "* Pytorch Turtorial: https://www.learnpytorch.io/00_pytorch_fundamentals/\n",
        "* Kaggel dataset for ideas: https://www.kaggle.com/code/enzou3/sartorius-mask-r-cnn\n",
        "  * build own method based on `find_best_thresholds()`\n",
        "\n",
        "\n",
        "\n",
        "  **Important:**\n",
        "  **Please check the Hyperparameters for this File because this allows you for example to run the project with limited images or load pretrained models**"
      ],
      "metadata": {
        "id": "q5rAGNxLJqnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 About the project\n",
        "\n",
        "Link to the project as follow: https://www.kaggle.com/competitions/sartorius-cell-instance-segmentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Main objectives:\n",
        "\n",
        "This Kaggle competition is about creating a computer program to identify and outline individual nerve cells in microscope images. These nerve cells are important for studying brain diseases like Alzheimer's and brain tumors, which are major health problems worldwide. Typically, scientists look at these cells using a microscope, but finding each cell in the images can be tough and takes a lot of time. Doing this accurately could help find new treatments for these diseases.\n",
        "\n",
        "The challenge is that current methods aren't very good at recognizing these nerve cells, especially a kind called neuroblastoma cells, which look very different from other cells and are hard to identify with existing tools.\n",
        "\n",
        "Sartorius, a company that supports science and medicine research, is sponsoring this competition. They want participants to develop a method that can automatically and precisely identify different types of nerve cells in images. This would be a big step forward in neurological research, making it easier for scientists to understand how diseases affect nerve cells and possibly leading to the discovery of new medications.\n",
        "\n",
        "\n",
        "\n",
        "Dataset:\n",
        "\n",
        "The Dataset containa at arround xx images for training and xx images for testing. the goal would be to train a model whoch is able to segment neuronal cells.\n",
        "\n",
        "\n",
        "The ground truth data to the images for training consist several meta data which includes also the masks for training the segmentation problem. These are specified field of ecah datapoint:\n",
        "\n",
        "\n",
        "* _id - unique identifier for object_\n",
        "\n",
        "* _annotation - run length encoded pixels for the identified neuronal cell_\n",
        "\n",
        "* _width - source image width_\n",
        "\n",
        "* _height - source image height_\n",
        "\n",
        "* _cell_type - the cell line_\n",
        "\n",
        "* _plate_time - time plate was created_\n",
        "\n",
        "* _sample_date - date sample was created_\n",
        "\n",
        "* _sample_id - sample identifier_\n",
        "\n",
        "* _elapsed_timedelta - time since first image taken of sample_\n",
        "\n"
      ],
      "metadata": {
        "id": "hCJr31AcKlyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a7vC51jVI7wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Preparations: Loading Dataset and install or import Packages"
      ],
      "metadata": {
        "id": "FUGWIF-fI8Vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install adn import third party packages / functions\n",
        "\n",
        "In this chapter we install the third party packages which maybe are not installed in the prebuild google collab or on your local system\n",
        "\n",
        "* Tqdm --> progress bar\n",
        "* gdown --> Import google drive package\n"
      ],
      "metadata": {
        "id": "JWwrrWRkPXSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown:\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    !pip install gdown\n",
        "\n",
        "# Install tqdm:\n",
        "try:\n",
        "    import tqdm\n",
        "except ImportError:\n",
        "    !pip install tqdm"
      ],
      "metadata": {
        "id": "FM3PVEqnPdwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General Import which are used in this file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os.path\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import requests\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "from sklearn.metrics import fbeta_score"
      ],
      "metadata": {
        "id": "9024sz-hPm4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Define custom functions for this Project:\n",
        "\n",
        "In this chapter we are defining custom functions which we are using throughout this project:\n",
        "\n",
        "* `show_train_time` function to show the time how long the coputation of the model takes\n",
        "* `folder_content` function to display what is inside a folder\n",
        "* `check_drop_image_existence` function to drop from the label dataset images which are not in the file system\n",
        "* `accuracy_fn` function for multi-label calssification problems\n",
        "* `plot_loss_values` for plotting the loss and accuracy to detect under or overfitting\n",
        "* `model_rating` gives back the rating of the model with accuracy and score for a given dataloader dataset\n",
        "* `make_pred` make predictions with a model based on test data\n",
        "* `combine_models_predictions_2` combines the results of two models with the size of 4 and 13 labels to a result of 17 labels\n",
        "* `make_pred_combined` make predctions for the combined approach  with two models, one for the weather labels and another for the other labelsm\n",
        "\n"
      ],
      "metadata": {
        "id": "woSCLCKBPwKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the timing function:\n",
        "from timeit import default_timer as timer\n",
        "def show_train_time(start:float,\n",
        "                     end:float,\n",
        "                     device: torch.device = None):\n",
        "  \"\"\"Show differnences between start and end time for calculation the performance of a pytorch model\"\"\"\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "PLL3UP7MP0LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def folder_content(directory_path):\n",
        "  \"\"\"\n",
        "  Iterating thorugh all folders in the path and display the content.\n",
        "  Args:\n",
        "    directory_path --> Path to start iteration\n",
        "\n",
        "  Returns:\n",
        "    Show information about:\n",
        "      subdiretories in dir_path\n",
        "      number of files in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(directory_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "e1H0AP2eP52F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_drop_image_existence(label_data: pd.DataFrame, images_dir : string):\n",
        "  \"\"\"\n",
        "  Method which cleans the label dataframe by checking the existence of images\n",
        "  \"\"\"\n",
        "  data_frame = label_data\n",
        "  index_drop = []\n",
        "  #print(f\"Check Images in Folder {images_dir}\")\n",
        "  for index, row in tqdm(data_frame.iterrows(), desc=\"Checking if File Exists.....\"):\n",
        "    path_to_check = os.path.join(images_dir, row['image_name'])\n",
        "    file_exists = os.path.isfile(path_to_check)\n",
        "    if file_exists == False:\n",
        "      # File does not exist, drop row from dataset\n",
        "      #print(f\"File: {row['image_name']} in Label file does not exist as image and will be deleted from the label file\")\n",
        "      index_drop.append(index)\n",
        "\n",
        "  #Drop all rows in index_drop\n",
        "  for index in tqdm(index_drop, desc=\"Deleting rows in label dataset.....\"):\n",
        "    data_frame.drop(index, inplace=True)\n",
        "  return data_frame"
      ],
      "metadata": {
        "id": "LIb-468qP_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformation functions\n",
        "\n",
        "# Adjustments to PyTorch transformation classes to accommodate masks and targets\n",
        "# Inspired by Abishek and DATAISTA0's approach\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "class FlipVertically:\n",
        "    def __init__(self, likelihood):\n",
        "        self.likelihood = likelihood\n",
        "\n",
        "    def __call__(self, pic, label):\n",
        "        if random.random() < self.likelihood:\n",
        "            pic_height, pic_width = pic.shape[-2:]\n",
        "            pic = pic.flip(-2)\n",
        "            box = label[\"boxes\"]\n",
        "            box[:, [1, 3]] = pic_height - box[:, [3, 1]]\n",
        "            label[\"boxes\"] = box\n",
        "            label[\"masks\"] = label[\"masks\"].flip(-2)\n",
        "        return pic, label\n",
        "\n",
        "class FlipHorizontally:\n",
        "    def __init__(self, likelihood):\n",
        "        self.likelihood = likelihood\n",
        "\n",
        "    def __call__(self, pic, label):\n",
        "        if random.random() < self.likelihood:\n",
        "            pic_height, pic_width = pic.shape[-2:]\n",
        "            pic = pic.flip(-1)\n",
        "            box = label[\"boxes\"]\n",
        "            box[:, [0, 2]] = pic_width - box[:, [2, 0]]\n",
        "            label[\"boxes\"] = box\n",
        "            label[\"masks\"] = label[\"masks\"].flip(-1)\n",
        "        return pic, label\n",
        "\n",
        "\n",
        "class ConvertToTensor:\n",
        "    \"\"\"Convert PIL Image to Tensor.\"\"\"\n",
        "    def __call__(self, pic, label):\n",
        "        pic = F.to_tensor(pic)\n",
        "        return pic, label\n",
        "\n",
        "class Normalize:\n",
        "    \"\"\"Normalize Tensor image with mean and standard deviation.\"\"\"\n",
        "    def __call__(self, pic, label):\n",
        "        pic = F.normalize(pic, mean=RESNET_MEAN, std=RESNET_STD)\n",
        "        return pic, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_transforms(training):\n",
        "    base_transforms = [ConvertToTensor()]\n",
        "    if training:\n",
        "        # Augmentations for the training dataset\n",
        "        augmentations = [\n",
        "            FlipHorizontally(likelihood=0.5),  # Assuming a 50% chance to apply\n",
        "            FlipVertically(likelihood=0.5),  # Assuming a 50% chance to apply\n",
        "        ]\n",
        "        base_transforms.extend(augmentations)\n",
        "\n",
        "    # Always apply normalization last\n",
        "    base_transforms.append(Normalize())\n",
        "\n",
        "    return Compose(base_transforms)\n",
        "\n",
        "\n",
        "class Compose:\n",
        "    def __init__(self, ops):\n",
        "        self.ops = ops\n",
        "\n",
        "    def __call__(self, pic, label):\n",
        "        for operation in self.ops:\n",
        "            pic, label = operation(pic, label)\n",
        "        return pic, label\n",
        "\n",
        "\n",
        "# def transforms(training):\n",
        "#     base_transforms = [ConvertToTensor()]\n",
        "#     if NORMALIZE:\n",
        "#         base_transforms.append(Standardize())\n",
        "\n",
        "#     # Augmentations only for the training set\n",
        "#     if training:\n",
        "#         augmentations = [\n",
        "#             FlipHorizontally(0.5),\n",
        "#             FlipVertically(0.5),\n",
        "#         ]\n",
        "#         if NORMALIZE:  # Avoid appending Standardize twice if already added\n",
        "#             base_transforms.extend(augmentations)\n",
        "#         else:\n",
        "#             base_transforms += [Standardize()] + augmentations\n",
        "\n",
        "#     return ChainTransforms(base_transforms)\n"
      ],
      "metadata": {
        "id": "962ikXQk-rmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "RESNET_MEAN = [0.485, 0.456, 0.406]\n",
        "RESNET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "class ConvertToTensor:\n",
        "    \"\"\"Convert PIL Image to Tensor.\"\"\"\n",
        "    def __call__(self, pic, label):\n",
        "        pic = F.to_tensor(pic)\n",
        "        return pic, label\n",
        "\n",
        "class Normalize:\n",
        "    \"\"\"Normalize Tensor image with mean and standard deviation.\"\"\"\n",
        "    def __call__(self, pic, label):\n",
        "        pic = F.normalize(pic, mean=RESNET_MEAN, std=RESNET_STD)\n",
        "        return pic, label\n",
        "\n",
        "def get_transforms(training):\n",
        "    transforms = [ConvertToTensor()]\n",
        "    if training:\n",
        "        # Add any data augmentation transforms here for training\n",
        "        pass\n",
        "    transforms.append(Normalize())  # Normalize last to maintain [0,1] range for augmentations\n",
        "    return Compose(transforms)\n"
      ],
      "metadata": {
        "id": "1riSCmZ3BH67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decode_rle(rle_string, dimensions, fill_value=1):\n",
        "    \"\"\"\n",
        "    Decodes a run-length encoded string into a binary mask.\n",
        "\n",
        "    Parameters:\n",
        "    - rle_string: The run-length encoded string (e.g., \"2 3 5 2\" means start at 2, length 3, start at 5, length 2)\n",
        "    - dimensions: Tuple indicating the shape of the output mask (height, width)\n",
        "    - fill_value: The value to fill the mask with where the RLE indicates; default is 1\n",
        "\n",
        "    Returns:\n",
        "    - A 2D numpy array representing the mask, where filled areas are marked with `fill_value` and the rest are 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the RLE string into a list of strings, then convert every string into an integer\n",
        "    numbers = [int(num) for num in rle_string.split()]\n",
        "\n",
        "    # Extract start positions and lengths from the list of numbers\n",
        "    start_positions = numbers[::2]\n",
        "    lengths = numbers[1::2]\n",
        "\n",
        "    # Calculate end positions for each run\n",
        "    end_positions = [start + length for start, length in zip(start_positions, lengths)]\n",
        "\n",
        "    # Initialize a flat image array with zeros\n",
        "    flat_image = np.zeros(dimensions[0] * dimensions[1], dtype=np.float32)\n",
        "\n",
        "    # Fill the specified positions in the flat image array\n",
        "    for start, end in zip(start_positions, end_positions):\n",
        "        flat_image[start:end] = fill_value\n",
        "\n",
        "    # Reshape the flat image array back into the specified dimensions\n",
        "    return flat_image.reshape(dimensions)\n",
        "\n"
      ],
      "metadata": {
        "id": "xoSpdK-tVsF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_rle(image_matrix):\n",
        "    \"\"\"\n",
        "    Encodes a binary image matrix into a run-length encoding string.\n",
        "\n",
        "    Parameters:\n",
        "    - image_matrix: A 2D numpy array representing the image to be encoded. Expected to contain binary values (0s and 1s).\n",
        "\n",
        "    Returns:\n",
        "    - A string representing the run-length encoded image, where each pair of numbers represents a start position and the length of consecutive 1s.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find indices of all 1s in the flattened image array\n",
        "    one_positions = np.where(image_matrix.flatten() == 1)[0]\n",
        "\n",
        "    # Initialize the list to hold the encoding\n",
        "    encoding = []\n",
        "\n",
        "    # Track the previous position to determine the start of a new run\n",
        "    previous_position = -2\n",
        "\n",
        "    # Iterate through all positions of 1s\n",
        "    for position in one_positions:\n",
        "        # If current position is not consecutive, start a new run\n",
        "        if position > previous_position + 1:\n",
        "            encoding.extend((position + 1, 0))  # +1 for 1-based indexing\n",
        "        encoding[-1] += 1  # Increment the length of the current run\n",
        "        previous_position = position\n",
        "\n",
        "    # Join all elements in the list as a string separated by spaces\n",
        "    return ' '.join(map(str, encoding))\n"
      ],
      "metadata": {
        "id": "wE_zDAJVVsIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def eliminate_overlaps(primary_mask, additional_masks):\n",
        "    \"\"\"\n",
        "    Removes pixels from the primary mask where it overlaps with any of the additional masks.\n",
        "\n",
        "    Parameters:\n",
        "    - primary_mask: A 2D numpy array representing the primary mask to be modified.\n",
        "    - additional_masks: A list of 2D numpy arrays, each representing a mask to check for overlaps with the primary mask.\n",
        "\n",
        "    Returns:\n",
        "    - The modified primary mask with overlapping pixels set to 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Iterate through each of the additional masks\n",
        "    for mask in additional_masks:\n",
        "        # Find overlapping pixels between the primary mask and the current additional mask\n",
        "        overlap = np.logical_and(primary_mask, mask)\n",
        "\n",
        "        # Check if there is any overlap\n",
        "        if np.sum(overlap) > 0:\n",
        "            # Set overlapping pixels in the primary mask to 0\n",
        "            primary_mask[overlap] = 0\n",
        "\n",
        "    # Return the modified primary mask\n",
        "    return primary_mask\n"
      ],
      "metadata": {
        "id": "vbdUWg99VsK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_boxed_image(coordinates):\n",
        "    \"\"\"\n",
        "    Draws a rectangular box on an image represented by a numpy array, based on given coordinates.\n",
        "\n",
        "    Parameters:\n",
        "    - coordinates: A list of four elements representing the box's corners [xmin, ymin, xmax, ymax].\n",
        "\n",
        "    Returns:\n",
        "    - A 2D numpy array (image) with dimensions IMG_HEIGHT x IMG_WIDTH,\n",
        "      where pixels inside the box are set to 1, and the rest are 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a blank image with zeros\n",
        "    image = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "    # Extract and convert the box coordinates to integers\n",
        "    xmin, ymin, xmax, ymax = map(int, coordinates)\n",
        "\n",
        "    # Draw top and bottom lines of the box\n",
        "    for x in range(xmin, xmax):\n",
        "        if xmin != 0 and xmax != IMG_WIDTH:  # Check to prevent drawing outside the image\n",
        "            image[ymin-1][x] = 1  # Draw top line\n",
        "            image[ymax-1][x] = 1  # Draw bottom line\n",
        "\n",
        "    # Draw left and right lines of the box\n",
        "    for y in range(ymin, ymax):\n",
        "        if ymin != 0 and ymax != IMG_HEIGHT:  # Check to prevent drawing outside the image\n",
        "            image[y][xmax-1] = 1  # Draw right line\n",
        "            image[y][xmin-1] = 1  # Draw left line\n",
        "\n",
        "    # Return the image with the drawn box\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "DNvz-_86VsNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def find_bounding_box(mask):\n",
        "    \"\"\"\n",
        "    Calculates the bounding box of a non-zero area in a mask.\n",
        "\n",
        "    Parameters:\n",
        "    - mask: A 2D numpy array representing the mask from which to calculate the bounding box.\n",
        "\n",
        "    Returns:\n",
        "    - A list containing the coordinates of the bounding box: [xmin, ymin, xmax, ymax].\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the indices of non-zero elements in the mask\n",
        "    nonzero_indices = np.where(mask)\n",
        "\n",
        "    # Calculate the minimum and maximum x (column) indices\n",
        "    left_bound = np.min(nonzero_indices[1])\n",
        "    right_bound = np.max(nonzero_indices[1])\n",
        "\n",
        "    # Calculate the minimum and maximum y (row) indices\n",
        "    top_bound = np.min(nonzero_indices[0])\n",
        "    bottom_bound = np.max(nonzero_indices[0])\n",
        "\n",
        "    # Return the bounding box coordinates\n",
        "    return [left_bound, top_bound, right_bound, bottom_bound]\n"
      ],
      "metadata": {
        "id": "nWfHEnarVsPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def merge_masks(mask_list, threshold):\n",
        "    \"\"\"\n",
        "    Merges multiple masks into a single image, assigning a unique integer to each mask's area based on its order in the list,\n",
        "    but only for mask values above a specified threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - mask_list: A list of 2D numpy arrays (masks) to be merged. Each mask should have the same dimensions.\n",
        "    - threshold: A value that defines the minimum intensity for pixels to be considered part of a mask.\n",
        "\n",
        "    Returns:\n",
        "    - A 2D numpy array where each pixel's value corresponds to the order of the mask it belongs to,\n",
        "      or 0 if it does not belong to any mask or does not meet the threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty image with the same dimensions as the masks\n",
        "    combined_image = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "    # Iterate through each mask with its order starting from 1\n",
        "    for order, mask in enumerate(mask_list, 1):\n",
        "        # Update the combined image with the order number where the mask exceeds the threshold\n",
        "        combined_image[mask > threshold] = order\n",
        "\n",
        "    return combined_image\n"
      ],
      "metadata": {
        "id": "y_O52C7PVsR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def merge_masks_and_boxes(mask_image, box_image):\n",
        "    \"\"\"\n",
        "    Combines mask and box images into one, ensuring that the boxes overlay the masks and\n",
        "    have the highest value present in the mask image.\n",
        "\n",
        "    Parameters:\n",
        "    - mask_image: A 2D numpy array representing multiple masks, where non-zero values indicate mask presence.\n",
        "    - box_image: A 2D numpy array representing multiple boxes, marked as 1's for box edges and 0's for the background.\n",
        "\n",
        "    Returns:\n",
        "    - A 2D numpy array where both boxes and masks are combined. Boxes will overlay masks with the highest mask value found.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a result image with zeros\n",
        "    combined_image = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "    # Find the highest value in the mask_image\n",
        "    max_mask_value = np.max(mask_image)\n",
        "\n",
        "    # First pass: Copy mask values to the combined image\n",
        "    for x in range(IMG_WIDTH):\n",
        "        for y in range(IMG_HEIGHT):\n",
        "            combined_image[y, x] = mask_image[y, x]\n",
        "\n",
        "    # Second pass: Overlay boxes on the combined image, setting them to the highest mask value\n",
        "    for x in range(IMG_WIDTH):\n",
        "        for y in range(IMG_HEIGHT):\n",
        "            if box_image[y, x] != 0:  # If there's a box pixel, overlay it with the max mask value\n",
        "                combined_image[y, x] = max_mask_value\n",
        "\n",
        "    return combined_image\n"
      ],
      "metadata": {
        "id": "5wb_ySmbVsUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_masks(predictions):\n",
        "    \"\"\"\n",
        "    Filters predicted masks based on a minimum score threshold for each mask and\n",
        "    a maximum threshold for pixel values, specific to each label.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: A dictionary containing 'masks', 'scores', and 'labels' from a prediction model.\n",
        "\n",
        "    Returns:\n",
        "    - A list of binary mask arrays that have passed the score and pixel value thresholds and have been filtered for overlaps.\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_masks = []  # Initialize the list to store filtered masks\n",
        "\n",
        "    # Iterate through each mask and its associated score and label\n",
        "    for index, mask in enumerate(predictions[\"masks\"]):\n",
        "        score = predictions[\"scores\"][index].cpu().item()  # Convert score tensor to a Python float\n",
        "        label = predictions[\"labels\"][index].cpu().item()  # Convert label tensor to a Python int\n",
        "\n",
        "        # Check if the score exceeds the minimum score for the given label\n",
        "        if score > MIN_SCORE_DICT[label]:\n",
        "            mask_array = mask.cpu().numpy().squeeze()  # Convert the mask to a numpy array and remove any extra dimensions\n",
        "            # Apply a threshold to create a binary mask where pixel values indicate high likelihood\n",
        "            high_confidence_mask = mask_array > MASK_THRESHOLD_DICT[label]\n",
        "            # Remove pixels in the current mask that overlap with any of the previously added masks\n",
        "            high_confidence_mask = remove_overlapping_pixels(high_confidence_mask, filtered_masks)\n",
        "            # Add the processed mask to the list of filtered masks\n",
        "            filtered_masks.append(high_confidence_mask)\n",
        "\n",
        "    return filtered_masks\n"
      ],
      "metadata": {
        "id": "HP8zXJ1-Xu7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2 Metrics\n",
        "\n",
        "The **Intersection over Union (IoU)** score, also known as the **Jaccard Index**, is a metric used in image segmentation to evaluate the accuracy of an object detector. It quantifies the precision of segmentation by measuring the overlap between the predicted segmentation area and the ground truth area.\n",
        "\n",
        "The IoU score is calculated as follows:\n",
        "\n",
        "$$ \\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\n",
        "\n",
        "- **Area of Overlap**: The intersection area where both the predicted segmentation and the ground truth agree on the presence of an object.\n",
        "- **Area of Union**: The combined area of the predicted segmentation and the ground truth, encompassing any part of the object present in either.\n",
        "\n",
        "IoU scores range from **0** to **1**, where a score of **1** indicates perfect agreement between the predicted segmentation and the ground truth, and a score of **0** indicates no overlap.\n",
        "\n",
        "This metric is pivotal for:\n",
        "- Evaluating the precision of object detections within an image.\n",
        "- Averaging performance across a dataset to gauge the general efficacy of a segmentation model.\n",
        "- Calculating critical detection task metrics like true positives, false positives, and false negatives.\n",
        "\n"
      ],
      "metadata": {
        "id": "T0fh3Eo1alY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_iou(ground_truth, predictions, verbose=0):\n",
        "    \"\"\"\n",
        "    Calculates the Intersection over Union (IoU) score between ground truth labels and predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    - ground_truth (np.array): The ground truth labels as a 2D numpy array.\n",
        "    - predictions (np.array): The predicted labels as a 2D numpy array.\n",
        "    - verbose (int, optional): Verbosity level; if > 0, prints the number of unique objects in both ground truth and predictions.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: An IoU score matrix of size [number of true objects] x [number of predicted objects].\n",
        "    \"\"\"\n",
        "\n",
        "    # Count the number of unique objects/labels in both ground truth and predictions\n",
        "    num_true_objects = len(np.unique(ground_truth))\n",
        "    num_pred_objects = len(np.unique(predictions))\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Number of true objects: {num_true_objects}\")\n",
        "        print(f\"Number of predicted objects: {num_pred_objects}\")\n",
        "\n",
        "    # Compute the intersection for all object combinations\n",
        "    intersection_matrix = np.histogram2d(ground_truth.flatten(), predictions.flatten(), bins=(num_true_objects, num_pred_objects))[0]\n",
        "\n",
        "    # Calculate the area sizes for true and predicted objects\n",
        "    area_ground_truth = np.histogram(ground_truth, bins=num_true_objects)[0]\n",
        "    area_predictions = np.histogram(predictions, bins=num_pred_objects)[0]\n",
        "\n",
        "    # Reshape for broadcasting to compute union\n",
        "    area_ground_truth = np.expand_dims(area_ground_truth, -1)\n",
        "    area_predictions = np.expand_dims(area_predictions, 0)\n",
        "\n",
        "    # Compute the union for all object combinations\n",
        "    union_matrix = area_ground_truth + area_predictions - intersection_matrix\n",
        "\n",
        "    # Exclude background from consideration\n",
        "    intersection_matrix = intersection_matrix[1:, 1:]  # Exclude background\n",
        "    union_matrix = union_matrix[1:, 1:]  # Exclude background\n",
        "    union_matrix[union_matrix == 0] = 1e-9  # Prevent division by zero\n",
        "\n",
        "    # Calculate IoU by dividing intersection by union for all object combinations\n",
        "    iou_scores = intersection_matrix / union_matrix\n",
        "\n",
        "    return iou_scores\n"
      ],
      "metadata": {
        "id": "iwbNv7LzXvGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_precision(threshold, iou_matrix):\n",
        "    \"\"\"\n",
        "    Calculates precision metrics at a specified IoU threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - threshold (float): The IoU threshold for determining if a detection is a true positive.\n",
        "    - iou_matrix (np.array): The IoU score matrix between ground truth and predicted objects.\n",
        "\n",
        "    Returns:\n",
        "    - int: The number of true positives (TP) at the given threshold.\n",
        "    - int: The number of false positives (FP) at the given threshold.\n",
        "    - int: The number of false negatives (FN) at the given threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine matches based on the IoU threshold\n",
        "    is_match = iou_matrix > threshold\n",
        "\n",
        "    # True positives: Predictions that match a ground truth object exactly once\n",
        "    true_positives_count = np.sum(is_match, axis=1) == 1\n",
        "\n",
        "    # False positives: Predictions that do not match any ground truth object\n",
        "    false_positives_count = np.sum(is_match, axis=0) == 0\n",
        "\n",
        "    # False negatives: Ground truth objects not matched by any prediction\n",
        "    false_negatives_count = np.sum(is_match, axis=1) == 0\n",
        "\n",
        "    # Sum up the counts for TP, FP, FN\n",
        "    tp = np.sum(true_positives_count)\n",
        "    fp = np.sum(false_positives_count)\n",
        "    fn = np.sum(false_negatives_count)\n",
        "\n",
        "    return tp, fp, fn\n"
      ],
      "metadata": {
        "id": "I8wSH03vXvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_mean_average_precision(ground_truth_masks, predicted_masks, verbose=0):\n",
        "    \"\"\"\n",
        "    Calculates the mean Average Precision (mAP) for a set of ground truth and predicted masks,\n",
        "    where each mask represents segmented objects with unique values and the background is 0.\n",
        "\n",
        "    Parameters:\n",
        "    - ground_truth_masks (list of np.array): List of 2D numpy arrays representing ground truth masks.\n",
        "    - predicted_masks (list of np.array): List of 2D numpy arrays representing predicted masks.\n",
        "    - verbose (int, optional): Level of verbosity for printing detailed info. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "    - float: The mean Average Precision (mAP) score.\n",
        "    \"\"\"\n",
        "    # Compute IoU for each pair of ground truth and predicted masks\n",
        "    iou_scores = [compute_iou(truth, pred, verbose) for truth, pred in zip(ground_truth_masks, predicted_masks)]\n",
        "\n",
        "    # Initialize list to hold precision scores for different thresholds\n",
        "    precision_scores = []\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
        "\n",
        "    # Evaluate precision at different IoU thresholds from 0.5 to 0.95 with step of 0.05\n",
        "    for threshold in np.arange(0.5, 1.0, 0.05):\n",
        "        total_tp, total_fp, total_fn = 0, 0, 0  # Initialize counters for true positives, false positives, false negatives\n",
        "\n",
        "        # Calculate precision metrics for each IoU matrix\n",
        "        for iou_matrix in iou_scores:\n",
        "            tp, fp, fn = precision_at(threshold, iou_matrix)\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "\n",
        "        # Compute precision for the current threshold\n",
        "        precision = total_tp / (total_tp + total_fp + total_fn)\n",
        "        precision_scores.append(precision)\n",
        "\n",
        "        if verbose:\n",
        "            # Print detailed precision info at the current threshold\n",
        "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(threshold, total_tp, total_fp, total_fn, precision))\n",
        "\n",
        "    # Calculate the mean of the collected precision scores\n",
        "    mean_ap = np.mean(precision_scores)\n",
        "\n",
        "    if verbose:\n",
        "        # Print the overall mean Average Precision\n",
        "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(mean_ap))\n",
        "\n",
        "    return mean_ap\n"
      ],
      "metadata": {
        "id": "HzSaR2E6XvLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def calculate_average_iou_score(dataset, model):\n",
        "    \"\"\"\n",
        "    Calculates the average Intersection over Union (IoU) mean Average Precision (mAP) score across a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The dataset to evaluate, where each item is a tuple of an image and its associated targets.\n",
        "    - model: The prediction model to use for generating mask predictions.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average IoU mAP score for the dataset.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_iou_score = 0  # Initialize total IoU score\n",
        "\n",
        "    # Iterate over all items in the dataset\n",
        "    for img, targets in tqdm(dataset, desc=\"Evaluating dataset\"):\n",
        "        with torch.no_grad():  # Disable gradient calculation for inference\n",
        "            predictions = model([img.to(DEVICE)])[0]  # Perform prediction\n",
        "\n",
        "        # Combine ground truth masks using a fixed threshold\n",
        "        ground_truth_masks = combine_masks(targets['masks'], 0.5)\n",
        "\n",
        "        # Determine the most frequent predicted label\n",
        "        label_counts = pd.Series(predictions['labels'].cpu().numpy()).value_counts()\n",
        "        most_frequent_label = label_counts.sort_values().index[-1]\n",
        "\n",
        "        # Get the mask threshold for the most frequent label\n",
        "        mask_threshold = MASK_THRESHOLD_DICT[most_frequent_label]\n",
        "\n",
        "        # Combine predicted masks using the determined threshold\n",
        "        predicted_masks = combine_masks(get_filtered_masks(predictions), mask_threshold)\n",
        "\n",
        "        # Compute IoU mAP score for the current image and update the total score\n",
        "        total_iou_score += iou_map([ground_truth_masks], [predicted_masks])\n",
        "\n",
        "    # Calculate the average IoU mAP score across the dataset\n",
        "    average_iou_score = total_iou_score / len(dataset)\n",
        "\n",
        "    return average_iou_score\n"
      ],
      "metadata": {
        "id": "cJNEBrUhXvNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zn7ZlNIIXvP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYz8TdxyVsWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eg1WWNkNVsZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Checking for GPU and device agnostic code (Cuda(Nvidia / Apple Silicon)\n",
        "\n",
        "In this chapter we are checking if Hardware from Nvidia (Cuda framework) pr Apple Silicon (M1-M3) is available and switching the device"
      ],
      "metadata": {
        "id": "Q6-lqvEeQJOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup device agnostic code\n",
        "import torch\n",
        "device=\"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "  print(\"Metal available with Apple Silicon GPU\")\n",
        "  device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "  print(\"Cuda available with Nvidia GPU\")"
      ],
      "metadata": {
        "id": "0f-zw1nnQNZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Define Hyperparameter for Project:\n",
        "Her we have the hyperparameters for all three models:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GyIPTYl_QXG_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZodKdmiQZdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Downloading the dataset to Google Colab\n",
        "\n",
        "In this chapter we are downloading the dataset from a public Google Drive link to this colab instance. This is necessary to decrease the request time per image to the dataset:"
      ],
      "metadata": {
        "id": "B_llEUiIQjVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "# Setup paths and folders names and urls\n",
        "data_path = Path(\"dataset\")\n",
        "download_path = Path(\"kaggledownload\")\n",
        "\n",
        "dataset_url = 'https://drive.google.com/uc?id=1syZoLGGeFiFErCFL_iI1VO_4k2jLEaPv&confirm=t'\n",
        "\n",
        "\n",
        "# If the image folder doesn't exist, download it\n",
        "if data_path.is_dir():\n",
        "    print(f\"{data_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {data_path} directory, creating one...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    download_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "gdown.download(dataset_url, str(download_path / \"sartorius-cell-instance-segmentation.zip\"), quiet=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dRgnuDd3QkU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile(str(download_path / \"sartorius-cell-instance-segmentation.zip\"), \"r\") as zip_ref:\n",
        "    print(\"Unzipping train dataset data...\")\n",
        "    zip_ref.extractall(data_path)"
      ],
      "metadata": {
        "id": "W1VPnCiJQycG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Dataset preparation\n",
        "\n",
        "In this chapter we are preparing our dataset, that we are able to load it later on into a Pytorch Dataloader:\n",
        "\n",
        "* Load dataset from path\n"
      ],
      "metadata": {
        "id": "TNXyajeuQ5lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageCellDataset(Dataset):\n",
        "    def __init__(self, directory_path, data_frame, transformation_steps=None):\n",
        "        self.transformation_steps = transformation_steps\n",
        "        self.directory_path = directory_path\n",
        "        self.data_frame = data_frame\n",
        "        self.img_height = IMG_HEIGHT\n",
        "        self.img_width = IMG_WIDTH\n",
        "        self.cells_data = collections.defaultdict(dict)\n",
        "\n",
        "        grouped_df = self.data_frame.groupby(['id', 'cell_type'])['annotation'].apply(list).reset_index()\n",
        "\n",
        "        for idx, record in grouped_df.iterrows():\n",
        "            self.cells_data[idx] = {\n",
        "                'cell_id': record['id'],\n",
        "                'file_path': os.path.join(self.directory_path, record['id'] + '.png'),\n",
        "                'cell_annotations': record[\"annotation\"],\n",
        "                'type': CELL_TYPE_DICT[record[\"cell_type\"]]\n",
        "            }\n",
        "\n",
        "\n",
        "    # get the item and load annotations (mask) accordingly\n",
        "    def __getitem__(self, index):\n",
        "        cell_data = self.cells_data[index]\n",
        "        image_file = cell_data[\"file_path\"]\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "        num_objects = len(cell_data['cell_annotations'])\n",
        "        masks = np.zeros((num_objects, self.img_height, self.img_width), dtype=np.uint8)\n",
        "        bounding_boxes = []\n",
        "\n",
        "        for i, annotation in enumerate(cell_data['cell_annotations']):\n",
        "            decoded_mask = rle_decode(annotation, (self.img_height, self.img_width))\n",
        "            decoded_mask = Image.fromarray(decoded_mask)\n",
        "            decoded_mask = np.array(decoded_mask) > 0\n",
        "            masks[i, :, :] = decoded_mask\n",
        "            bounding_boxes.append(get_box(decoded_mask))\n",
        "\n",
        "        # Prepare labels based on cell type\n",
        "        labels = [cell_data[\"type\"] for _ in range(num_objects)]\n",
        "        bounding_boxes = torch.as_tensor(bounding_boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        cell_id_tensor = torch.tensor([index])\n",
        "        areas = (bounding_boxes[:, 3] - bounding_boxes[:, 1]) * (bounding_boxes[:, 2] - bounding_boxes[:, 0])\n",
        "        not_crowded = torch.zeros((num_objects,), dtype=torch.int64)\n",
        "\n",
        "        target_dict = {\n",
        "            'boxes': bounding_boxes,\n",
        "            'labels': labels,\n",
        "            'masks': masks,\n",
        "            'image_id': cell_id_tensor,\n",
        "            'area': areas,\n",
        "            'iscrowd': not_crowded\n",
        "        }\n",
        "\n",
        "        if self.transformation_steps:\n",
        "            image, target_dict = self.transformation_steps(image, target_dict)\n",
        "        return image, target_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cells_data)\n"
      ],
      "metadata": {
        "id": "hfzblgr-dcBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(TRAIN_CSV, nrows=5000 if TEST else None)\n",
        "ds_train = ImageCellDataset(TRAIN_PATH, df_train, transforms=get_transform(train=True))\n",
        "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "cxXG698SdcEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_sampled_images(columns_count):\n",
        "    '''\n",
        "    columns_count : number of images to display\n",
        "    This function displays a 2-row plot with 'columns_count' images on each row from the training dataset.\n",
        "    The first row shows the images, and the second row shows the corresponding masks and boxes.\n",
        "    '''\n",
        "    sample_indexes = random.sample(range(100), columns_count)\n",
        "    figure, axis = plt.subplots(2, columns_count, figsize=(20, 6))\n",
        "\n",
        "    for column in range(columns_count):\n",
        "        image, annotations = ds_train[sample_indexes[column]]\n",
        "        all_masks = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
        "        all_boxes = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
        "        axis[0][column].set_title(f\"Image {sample_indexes[column]}\")\n",
        "        axis[0][column].imshow(image.numpy().transpose((1, 2, 0)))\n",
        "        axis[0][column].axis(\"off\")\n",
        "\n",
        "        for mask in annotations['masks']:\n",
        "            bounding_box = get_box(mask)\n",
        "            all_boxes = np.logical_or(all_boxes, draw_box(bounding_box))\n",
        "\n",
        "        all_masks = combine_masks(annotations['masks'], 0.5)\n",
        "\n",
        "        axis[1][column].set_title(f\"{sample_indexes[column]} {DICT_TO_CELL[annotations['labels'][0].item()]} mask\")\n",
        "        combined_detections = combine_masks_boxes(all_masks, all_boxes)\n",
        "        axis[1][column].imshow(combined_detections)\n",
        "        axis[1][column].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example of how to call the function\n",
        "display_sampled_images(4)\n"
      ],
      "metadata": {
        "id": "x2Y71TsHdcHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer Baseline"
      ],
      "metadata": {
        "id": "loGk3ex0dcJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "class TransformerSegmentationModel(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, num_classes, dim, depth, heads, mlp_dim, decoder_layers):\n",
        "        super(TransformerSegmentationModel, self).__init__()\n",
        "        assert img_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        patch_dim = patch_size * patch_size * 3  # Assuming 3 channels (RGB)\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Positional embeddings for encoder and decoder\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        # Transformer Decoder\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=decoder_layers)\n",
        "\n",
        "        # Decoder for upsampling\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(dim, dim // 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(dim // 2, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.reshape(b, c, h // self.patch_size, self.patch_size, w // self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 4, 3, 5, 1).flatten(1, 2)\n",
        "        patches = self.patch_to_embedding(x)\n",
        "        patches += self.pos_embedding\n",
        "\n",
        "        # Encoder\n",
        "        encoded_patches = self.transformer_encoder(patches)\n",
        "\n",
        "        # Decoder (Transformer Decoder followed by Convolutional Upsampling)\n",
        "        decoded_patches = self.transformer_decoder(encoded_patches, encoded_patches)\n",
        "        decoded_patches = decoded_patches.permute(0, 2, 1).view(b, -1, h // self.patch_size, w // self.patch_size)\n",
        "\n",
        "        # Upsampling to original size\n",
        "        out = self.decoder(decoded_patches)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example model instantiation\n",
        "IMG_SIZE = 256  # Example image size\n",
        "NUM_CLASSES = 10  # Example number of segmentation classes\n",
        "model = TransformerSegmentationModel(\n",
        "    img_size=IMG_SIZE, patch_size=16, num_classes=NUM_CLASSES, dim=512, depth=6, heads=8, mlp_dim=2048, decoder_layers=6\n",
        ")\n"
      ],
      "metadata": {
        "id": "xD_cuL4D9F7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "# class SimpleVisionTransformer(nn.Module):\n",
        "#     def __init__(self, img_size, patch_size, num_classes, dim, depth, heads, mlp_dim):\n",
        "#         super(SimpleVisionTransformer, self).__init__()\n",
        "#         assert img_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "#         num_patches = (img_size // patch_size) ** 2\n",
        "#         patch_dim = patch_size * patch_size * 3  # Assuming 3 channels (RGB)\n",
        "#         self.patch_size = patch_size\n",
        "\n",
        "#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))\n",
        "#         self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "#         self.transformer = TransformerEncoder(\n",
        "#             TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim), num_layers=depth\n",
        "#         )\n",
        "\n",
        "#         self.to_cls_token = nn.Identity()\n",
        "\n",
        "#         self.decoder = nn.Sequential(\n",
        "#           nn.Linear(dim, mlp_dim),\n",
        "#           nn.ReLU(),\n",
        "#           nn.Linear(mlp_dim, num_classes * patch_size * patch_size)  # For each patch, predict class for each pixel\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b, c, h, w = x.shape\n",
        "#         x = x.reshape(b, c, h // self.patch_size, self.patch_size, w // self.patch_size, self.patch_size)\n",
        "#         x = x.permute(0, 2, 4, 3, 5, 1).flatten(1, 2)\n",
        "#         patches = self.patch_to_embedding(x)\n",
        "#         cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "#         x = torch.cat((cls_tokens, patches), dim=1)\n",
        "#         x += self.pos_embedding[:, :(patches.shape[1] + 1)]\n",
        "#         x = self.transformer(x)\n",
        "#         x = self.to_cls_token(x[:, 0])\n",
        "#         x = self.decoder(x)\n",
        "#         return x.view(b, num_classes, h, w)  # Output shape for segmentation map\n",
        "\n",
        "# # Example model instantiation\n",
        "# model = SimpleVisionTransformer(\n",
        "#     img_size=IMG_SIZE, patch_size=16, num_classes=NUM_CLASSES, dim=128, depth=4, heads=4, mlp_dim=512\n",
        "# )\n"
      ],
      "metadata": {
        "id": "-d8y9dGYdcM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeXvVMWgdcPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import time\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Your dataset and DataLoader setup\n",
        "# dataset_train = CellDataset(image_dir=TRAIN_DIR, df=train_df, transforms=transforms(train=True))\n",
        "# dl_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "# # Assuming your model is already defined as 'model'\n",
        "# model.to(DEVICE)\n",
        "\n",
        "# # Optimizer and scheduler setup\n",
        "# params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "# n_batches = len(dl_train)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(1, NUM_EPOCHS + 1):\n",
        "#     print(f\"Epoch {epoch}/{NUM_EPOCHS}:\")\n",
        "#     time_start = time.time()\n",
        "\n",
        "#     model.train()\n",
        "#     loss_accum = 0.0\n",
        "#     # Assuming your model returns a dictionary with these keys or adjust as per your model's output\n",
        "#     loss_objective_accum = 0.0\n",
        "\n",
        "#     for batch_idx, (images, targets) in enumerate(dl_train, start=1):\n",
        "#         images = list(image.to(DEVICE) for image in images)\n",
        "#         # Adjust targets formatting as per your requirement\n",
        "#         targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(images)  # Your model's forward pass\n",
        "#         loss = loss_function(output, targets)  # Define your loss_function according to your model's output and targets\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         loss_accum += loss.item()\n",
        "#         # Example: Assuming your loss dictionary has 'loss_objective'\n",
        "#         # Adjust according to your actual loss components\n",
        "#         loss_objective_accum += loss_dict['loss_objective'].item() if 'loss_objective' in loss_dict else 0\n",
        "\n",
        "#         if batch_idx % 50 == 0:  # Logging interval\n",
        "#             print(f\"    [Batch {batch_idx:3d}/{n_batches:3d}] Batch loss: {loss.item():.3f}\")\n",
        "\n",
        "#     # Learning rate scheduler update\n",
        "#     if lr_scheduler is not None:\n",
        "#         lr_scheduler.step()\n",
        "\n",
        "#     avg_loss = loss_accum / n_batches\n",
        "#     avg_loss_objective = loss_objective_accum / n_batches\n",
        "\n",
        "#     elapsed_time = time.time() - time_start\n",
        "\n",
        "#     # Epoch-end summaries\n",
        "#     print(f\"[Epoch {epoch:2d}/{NUM_EPOCHS:2d}] Avg loss: {avg_loss:.3f}, Objective loss: {avg_loss_objective:.3f} [{elapsed_time:.0f} secs]\")\n",
        "#     torch.save(model.state_dict(), f\"model_epoch_{epoch}.pth\")\n",
        "\n",
        "# # Adjustments needed:\n",
        "# # - Ensure `loss_function` correctly computes the loss between your model's output and the target annotations.\n",
        "# # - If your model's output or target data structure is different, adjust the loss computation and data preparation steps accordingly.\n",
        "# # - Depending on your loss structure, you might need to add or remove lines related to specific loss components (e.g., 'loss_objective').\n"
      ],
      "metadata": {
        "id": "KbSxaynXpgMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = CellDataset(image_dir=TRAIN_DIR, df=train_df, transforms=transforms(train=True))\n",
        "dl_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "RGwvT3ovA29Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, optimizer, and loss function setup\n",
        "model.to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, targets in dl_train:\n",
        "        images = images.to(DEVICE)\n",
        "        masks = targets['masks'].to(DEVICE)  # Assuming masks are your target for segmentation\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = loss_function(output, masks.long())  # Ensure masks are in the correct dtype for CrossEntropyLoss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dl_train)}\")"
      ],
      "metadata": {
        "id": "uAVjNZfOpgUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShI4dSgypgWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdm1_U-EpgY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from timm.models import create_model\n",
        "from torch import nn\n",
        "\n",
        "# Example of instantiating a Swin Transformer model\n",
        "# This assumes you have a segmentation head ready or are using a framework that provides one\n",
        "model = create_model('swin_small_patch4_window7_224', pretrained=True, num_classes=NUM_CLASSES)  # NUM_CLASSES is your dataset's number of classes\n",
        "\n",
        "# Add a custom segmentation head if necessary (this is highly dependent on your setup and framework)\n",
        "model.head = CustomSegmentationHead(...)  # Define or configure your segmentation head here\n"
      ],
      "metadata": {
        "id": "MiR7XQ4BdcSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEuZy44UdcU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming 'CellDataset' is your dataset class\n",
        "dataset_train = CellDataset(image_dir=TRAIN_DIR, df=train_df, transforms=get_transform(train=True))\n",
        "dl_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "# Optimizer and scheduler setup remains the same\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for images, targets in dl_train:\n",
        "        images = list(img.to(DEVICE) for img in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n"
      ],
      "metadata": {
        "id": "9vGI15NPdcXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIHR8ptAoMFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ELEjKOSoMHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2q6ENRZ-oMJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrpPz34moMMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CR3RXmVtoMOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_rbG2jloMQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}